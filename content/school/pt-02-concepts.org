#+title: Concepts
#+description: Reading
#+colordes: #538cc6
#+slug: pt-02-concepts
#+weight: 2

* What is machine learning?

Machine learning (ML) can be defined as computer programs whose performance at a task improves with experience.

Since this experience comes in the form of *data*, ML consists of feeding vast amounts of data to algorithms to strengthen *pathways*.

{{<br>}}
{{<img src="https://imgs.xkcd.com/comics/machine_learning.png" title="" width="%" line-height="0.5rem">}}
from <a href="https://xkcd.com/">xkcd.com</a>
{{</img>}}

**** Example in image recognition

Coding all the possible ways—pixel by pixel—that a picture can represent a certain object is an impossibly large task. By feeding examples of images of that object to a neural network however, we can train it to recognize that object in images that it has never seen (without explicitly programming how it does this!).

{{<br>}}
{{<img src="https://imgs.xkcd.com/comics/machine_learning_captcha.png" title="" width="%" line-height="0.5rem">}}
from <a href="https://xkcd.com/">xkcd.com</a>
{{</img>}}

* Supervised learning

You have been doing supervised machine learning for years without thinking about it in those terms:

- *Regression* is a form of supervised learning with continuous output
- *Classification* is supervised learning with discrete output

Supervised learning uses training data in the form of example input/output \((x_i, y_i)\) pairs.

*/Goal:/*

If \(X\) is the space of inputs and \(Y\) the space of outputs, the goal is to find a function \(h\) so that\\
for each \(x_i \in X\), \(h_\theta(x_i)\) is a predictor for the corresponding value \(y_i\) \\
(\(\theta\) represents the set of parameters of \(h_\theta\)).

→ i.e. we want to find the relationship between inputs and outputs.

* Unsupervised learning

Here too, you are familiar with some form of unsupervised learning that you weren't thinking about in such terms:

*Clustering*, *social network analysis*, *market segmentation*, *PCA* ... are all forms of unsupervised learning.

Unsupervised learning uses unlabelled data (training set of \(x_i\)).

*/Goal:/*

Find structure within the data.

* Artificial neural networks

[[https://westgrid-ml.netlify.app/school/pt-03-nn.html][The next lesson]] will show you four excellent videos by [[https://www.3blue1brown.com/][3Blue1Brown]] on neural networks, so you will learn a lot about them then. But in short, artificial neural networks (ANN) are layered networks of units mimicking the concept of biological neurons: inputs are received by every unit, computed, then transmitted to other units. Experience strengthens some connections between units and weakens others.

In biological neurons, the information consists of action potentials (cell membrane rapid depolarization), in artificial ones, it is a weighted sum of inputs with an additional (possibly weighted) bias.

/Schematic of biological neuron:/
{{<br>}}
{{<img src="https://upload.wikimedia.org/wikipedia/commons/b/b5/Neuron.svg" title="" width="100%" line-height="2.5rem">}}
from <a href="https://commons.wikimedia.org/w/index.php?curid=1474927">Dhp1080, Wikipedia</a>
{{</img>}}

/Schematic of artificial neuron:/
{{<br>}}
{{<img src="/img/school/artificial_neuron_nw.png" title="" width="70%" line-height="0rem">}}
<center>{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}modified from <a href="https://royalsocietypublishing.org/doi/10.1098/rsta.2019.0163">O.C. Akgun & J. Mei 2019</a></center>
{{</img>}}

{{<2br>}}
While biological neurons are connected in intricate ways, artificial ones follow a layered structure. Another difference in complexity is in the number of units: the human brain has 65–90 billion neurons. ANN have much fewer units.

/Neurons in mouse cortex:/
{{<img src="/img/school/brain_neurons.jpg" title="" width="70%" line-height="2.5rem">}}
<center>{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<n>}}Neurons are in green, the dark branches are blood vessels <br>
{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}image by <a href="https://news.berkeley.edu/2020/03/19/high-speed-microscope-captures-fleeting-brain-signals/">Na Ji, UC Berkeley</a></center>
{{</img>}}

/Neural network with 2 hidden layers:/
{{<br>}}
{{<img src="/img/school/nn_multi_layer_nw.png" title="" width="80%" line-height="1.0rem">}}
<center>{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}from <a href="https://themaverickmeerkat.com/2020-01-10-TicTacToe/">The Maverick Meerkat</a></center>
{{</img>}}

{{<2br>}}
The information in biological neurons is an all-or-nothing electrochemical pulse or action potential. Greater stimuli don’t produce stronger signals but increase firing frequency. In contrast, artificial neurons pass the computation of their inputs through an activation function which can take any value.

/Threshold potential:/
{{<img src="/img/school/all_none_law_nw.png" title="" width="60%" line-height="0rem">}}
<center>{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}modified from <a href="https://commons.wikimedia.org/w/index.php?curid=78013076">Blacktc, Wikimedia</a></center>
{{</img>}}

{{<br>}}
/Some of the most common activation functions:/
{{<img src="/img/school/act_func_nw.png" title="" width="60%" line-height="2.0rem">}}
<center>{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}{{<m>}}from <a href="https://arxiv.org/abs/1908.08681">Diganta Misra 2019</a></center>
{{</img>}}

The process of learning in biological NN happens through neuron death or growth, and through the creation or loss of synaptic connections between neurons. In ANN, learning happens through optimization algorithms such as gradient descent which adjust the weights and biases connecting each layer of neurons over many iterations.

/Gradient descent:/
{{<img src="https://upload.wikimedia.org/wikipedia/commons/f/ff/Gradient_descent.svg" title="" width="100%" line-height="0.5rem">}}
from <a href="https://commons.wikimedia.org/w/index.php?curid=20569355">Olegalexandrov & Zerodamage, Wikipedia</a>
{{</img>}}

* Comments & questions
