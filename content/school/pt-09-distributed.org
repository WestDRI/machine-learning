#+title: Distributed computing with PyTorch
#+description: Reading
#+colordes: #538cc6
#+slug: pt-09-distributed
#+weight: 9

* Data parallelism

Run the same model on multiple GPUs.
Each GPU uses a different partition of the data.

Uses {{<c>}}torch.nn.DataParallel{{</c>}}.

{{<c>}}DataParallel{{</c>}} splits the data automatically and sends jobs to the GPUs: each GPU gets a different data partition and runs the model on it. Once all the jobs are done, {{<c>}}DataParallel{{</c>}} collects and compiles the results before returning the final output.

/Example:/

You have a mini-batch of size \(m\) and 2 GPUs. {{<c>}}DataParallel{{</c>}} sends \(m/2\) samples to one GPU and the other \(m/2\) samples to the other. Once they have ran the model on that data, {{<c>}}DataParallel{{</c>}} collects, compiles and returns the result.

Since each GPU runs the model on half the data, this goes a lot faster.

* Model parallelism

If the model is too large to fit on a single GPU.
The model is split between different GPUs.

* Distributed computing on different nodes


* Comments & questions
