#+title: Running ML jobs in Compute Canada clusters
#+description: Reading
#+colordes: #538cc6
#+slug: pt-08-hpc
#+weight: 8

/Note on notation: expressions between the {{<c>}}&lt;{{</c>}} and {{<c>}}&gt;{{</c>}} signs need to be replaced by the relevant information (without those signs)./

* Prepare your script

Throughout your script, make sure to have plots written to file and not displayed on screen.

You can develop your script directly in the cluster using an interactive job, or on your local machine (working on your machine at this stage, using very little data and few epochs—only to develop and test the code—is a convenient approach).

Ultimately, you will submit your script to {{<c>}}sbatch{{</c>}}.

* Prepare the data

The Compute Canada clusters are optimized for very large files and are slowed by large collections of small files. Datasets with many small files need to be turned into single-file archives with {{<c>}}tar{{</c>}}. Failing to do so will affect performance not just for you, but for all users of the cluster.

#+BEGIN_src sh
$ tar cf <data>.tar <path/to/dataset/directory>/*
#+END_src

/Notes:/

- If you want to also compress the files, replace {{<c>}}tar cf{{</c>}} with {{<c>}}tar czf{{</c>}}
- As a modern alternative to {{<c>}}tar{{</c>}}, you can use [[https://docs.computecanada.ca/wiki/Dar][Dar]]
# Note that there are ML datasets available on the Compute Canada clusters. Maybe the dataset you plan to use is already available.

* Copy files to the cluster

** Small files

If you need to copy files such as your script to the cluster, you can use {{<c>}}scp{{</c>}}.

***** From your computer

If you are in a local shell, run:

#+BEGIN_src sh
[local]$ scp </local/path/to/file>  <user>@<cluster>.computecanada.ca:<path/in/cluster>
#+END_src

/(Replace {{<c>}}&lt;user&gt;{{</c>}} by your user name and {{<c>}}&lt;cluster&gt;{{</c>}} by the name of the Compute Canada cluster hostname (e.g. beluga, cedar, graham).)/

***** From the cluster

If you are in a remote shell (through ssh), run:

#+BEGIN_src sh
[cluster]$ scp <user>@<cluster>.computecanada.ca:<cluster/path/to/file>  </local/path>
#+END_src

** Data

Use [[https://docs.computecanada.ca/wiki/Globus][Globus]] for large data transfers.

* Load necessary modules

To use Python for ML on the cluster, you will need to load the relevant modules.

This is done with the [[https://github.com/TACC/Lmod][Lmod]] tool through the [[https://docs.computecanada.ca/wiki/Utiliser_des_modules/en][module]] command. Here are some key [[https://lmod.readthedocs.io/en/latest/010_user.html][commands]]:

#+BEGIN_src sh
# Get help on the module command
$ module help

# List modules that are already loaded
$ module list

# See which modules are available for Python
$ module avail python

# Load Python, CUDA, and cuDNN
$ module load python/3.8.2 cudacore/.10.1.243 cuda/10 cudnn/7.6.5
#+END_src

NVIDIA [[https://developer.nvidia.com/cudnn][CUDA Deep Neural Network library (cuDNN)]] is a GPU-accelerated library of primitives for deep neural networks.

* GPU(s)

*** GPU type

Several Compute Canada clusters have GPUs. Their numbers and types differ:

{{<img src="/img/school/cc_gpu.png" title="" width="%" line-height="0.5rem">}}
from <a href="https://docs.computecanada.ca/wiki/Using_GPUs_with_Slurm">Compute Canada Wiki</a>
<br><br>
{{</img>}}

The default is {{<b>}}12G P100{{</b>}}, but you can request another type with {{<c>}}SBATCH --gres=gpu:&lt;type&gt;:&lt;number&gt;{{</c>}}

*** Number of GPU(s)

#+BEGIN_export html
<font color="#bf540c">Try running your model on <b>one</b> GPU first.</font>
#+END_export

It is very likely that you do not need more than one GPU. Asking for more than you need will greatly increase your waiting time until your job is run. The session on [[https://westgrid-ml.netlify.app/school/pt-11-distributed.html][distributed computing with PyTorch]] will help you see when you might benefit from several GPUs. In any event, you should test your model before asking for several GPUs.

*** CPU/GPU ratio

Here are Compute Canada recommendations:

*Béluga*:\\
No more than 10 CPU per GPU

*Cedar*:\\
P100 GPU: no more than 6 CPU per GPU\\
V100 GPU: no more than 8 CPU per GPU

*Graham*:\\
No more than 16 CPU per GPU

* Install Python wheels and test your code

You also need Python packages.

For this, create a virtual environment on compute nodes (this improves I/O performance) in which you install packages with {{<c>}}pip{{</c>}}.

#+BEGIN_box
*Do not use Anaconda* \\
While Anaconda is a great tool on personal computers, it is not an appropriate tool when working on the Compute Canada clusters: binaries are unoptimized for those clusters and library paths are inconsistent with their architecture. Anaconda installs packages in {{<b>}}$HOME{{</b>}} where it creates a very large number of small files. It can also create conflicts by modifying {{<b>}}.bashrc{{</b>}}
#+END_box

The variable {{<b>}}$SLURM_TMPDIR{{</b>}} is created by Slurm on the compute node where a job is running. Its path is {{<b>}}/localscratch/&lt;user&gt;.&lt;jobid&gt;.0{{</b>}}. Anything in it gets deleted when the job is done.

It may be a good idea to run an interactive job to create a requirements file and test your code:

** Start an interactive job

/Example:/

#+BEGIN_src sh
$ salloc --account=def-<user> --gres=gpu:1 --cpus-per-task=6 --mem=32000 --time=1:00
#+END_src

# Use pre-downloaded packages.

** Create a requirements file

Create a Python virtual environment:

#+BEGIN_src sh
$ virtualenv --no-download $SLURM_TMPDIR/env
#+END_src

Activate it:

#+BEGIN_src sh
$ source $SLURM_TMPDIR/env/bin/activate
#+END_src

Update pip:

#+BEGIN_src sh
(env) $ pip install --no-index --upgrade pip
#+END_src

Install the packages you need in the virtual environment:

#+BEGIN_src sh
(env) $ pip install --no-index torch torchvision
#+END_src

Save the requirements file:

#+BEGIN_src sh
(env) $ pip freeze > ~/<path/project/dir/>requirements.txt
#+END_src

** Try to run your code

Create a temporary data directory in {{<b>}}$SLURM_TMPDIR{{</b>}}:

#+BEGIN_src sh
(env) $ mkdir $SLURM_TMPDIR/data
#+END_src

Extract the data into it:

#+BEGIN_src sh
(env) $ tar xf ~/projects/def-<user>/<data>.tar -C $SLURM_TMPDIR/data
#+END_src

Play in Python to test your code:

#+BEGIN_src sh
(env) $ python
#+END_src

#+BEGIN_src python
>>> import torch
>>> ...
#+END_src

/Note:/ {{<s>}}if you want to exit the virtual environment:

#+BEGIN_src sh
(env) $ deactivate
#+END_src

* Job script

To submit a job to Slurm (the job scheduler used by the Compute Canada clusters), you need to write an {{<b>}}sbatch{{</b>}} script. Here is an example script:

#+BEGIN_src sh
#!/bin/bash
#SBATCH --job-name=<name>*			  # job name
#SBATCH --account=def-<user>
#SBATCH --time=<time>				  # max walltime in D-HH:MM or HH:MM:SS
#SBATCH --cpus-per-task=<number>      # number of cores
#SBATCH --gres=gpu:<type>:<number>    # type and number of GPU(s) per node
#SBATCH --mem=<mem>					  # max memory (default unit is MB) per node
#SBATCH --output=<file%j.out>*		  # file name for the output
#SBATCH --error=<file%j.err>*		  # file name for errors
					                  # %j gets replaced by the job number
#SBATCH --mail-user=<email_address>*
#SBATCH --mail-type=ALL*

# Load modules
module load python/3.8.2 cudacore/.10.1.243 cuda/10 cudnn/7.6.5

# Create variable with the directory for your ML project
SOURCEDIR=~/<path/project/dir>

# Create and activate a virtual environment on compute node
virtualenv --no-download $SLURM_TMPDIR/env
source $SLURM_TMPDIR/env/bin/activate

# Install Python packages
pip install --no-index -r $SOURCEDIR/requirements.txt

# Transfer and extract data
mkdir $SLURM_TMPDIR/data
tar xf ~/projects/def-<user>/<data>.tar -C $SLURM_TMPDIR/data

# Run Python script on the data
python $SOURCEDIR/<mlscript>.py $SLURM_TMPDIR/data
#+END_src

/Notes:/

- If you compressed your data with {{<c>}}tar czf{{</c>}}, you need to extract it with {{<c>}}tar xzf{{</c>}}
- {{<c>}}SBATCH{{</c>}} options marked with a {{<c>}}*{{</c>}} are optional
- There are various other options for [[https://docs.computecanada.ca/wiki/Running_jobs#Email_notification][email notifications]].

* Job handling

**** Submit job

#+BEGIN_src sh
$ cd </dir/containing/job>
$ sbatch <jobscript>.sh
#+END_src

**** Check job status

#+BEGIN_src sh
$ sq
#+END_src

{{<b>}}PD{{</b>}} = pending\\
{{<b>}}R{{</b>}} = running

**** Cancel job

#+BEGIN_src sh
$ scancel <jobid>
#+END_src

**** Display efficiency measures of completed job

#+BEGIN_src sh
$ seff <jobid>
#+END_src

* Checkpoints

Long jobs should have a checkpoint at least every 24 hours. This ensures that an outage won't lead to days of computation lost and it will help get the job started sooner by the scheduler.

For instance, you might want to have checkpoints every {{<b>}}n{{</b>}} epochs (choose {{<b>}}n{{</b>}} so that {{<b>}}n{{</b>}} epochs take less than 24 hours to run).

In PyTorch, you can create dictionaries with all the information necessary and save them as {{<b>}}.tar{{</b>}} files with {{<c>}}torch.save(){{</c>}}. You can then load them back with {{<c>}}torch.load(){{</c>}}.

The information you want to save in each checkpoint includes the model’s state_dict, the optimizer’s state_dict, the epoch at which you stopped, the latest training loss, and anything else needed to restart training where you left off.

/Example:/

Saving a checkpoint during training could look something like this:

#+BEGIN_src python
torch.save({
    'epoch': <last epoch run>,
    'model_state_dict': net.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'loss': <latest loss>,
}, <path/to/file.tar>)
#+END_src

To restart, initialize the model and optimizer, load the dictionary, and resume training:

#+BEGIN_src python
# Initialize the model and optimizer
model = <your model>
optimizer = <your optimizer>

# Load the dictionary
checkpoint = torch.load(<path/to/file.tar>)
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
epoch = checkpoint['epoch']
loss = checkpoint['loss']

# Resume training
model.train()
#+END_src

* Running several similar jobs

A number of ML tasks (e.g. [[https://en.wikipedia.org/wiki/Hyperparameter_optimization][hyperparameter optimization]]) require running several instances of similar jobs. Grouping them into a single job with [[https://docs.computecanada.ca/wiki/GLOST][GLOST]] or [[https://docs.computecanada.ca/wiki/GNU_Parallel][GNU Parallel]] reduces the stress on the scheduler.

* Using TensorBoard in the cluster

If you want to visually track your model metrics (e.g. loss, accuracy, model graph, etc.) with [[https://github.com/tensorflow/tensorboard][TensorBoard]], you need to launch it in the background (using {{<c>}}&{{</c>}}) before your python script by adding to your {{<b>}}sbatch{{</b>}} script:

#+BEGIN_src sh
tensorboard --logdir=/tmp/<your log dir> --host 0.0.0.0 &
#+END_src

So our example script would look like:

#+BEGIN_src sh
#!/bin/bash
#SBATCH ...
...

tensorboard --logdir=/tmp/<your log dir> --host 0.0.0.0 &
python $SOURCEDIR/<mlscript>.py $SLURM_TMPDIR/data
#+END_src

Once the job is running, you need to create a connection between the compute node running TensorBoard and your computer.

First, you need to find the hostname of the compute node running the Tensorboard server. This is the value under {{<b>}}NODELIST{{</b>}} for your job when you run:

#+BEGIN_src sh
$ sq
#+END_src

Then, *from your computer*, enter this {{<c>}}ssh{{</c>}} command:

#+BEGIN_src sh
[local]$ ssh -N -f -L localhost:6006:<node hostname>:6006 <user>@<cluster>.computecanada.ca
#+END_src

/(Replace {{<c>}}&lt;node hostname&gt;{{</c>}} by the compute node hostname you just identified, {{<c>}}&lt;user&gt;{{</c>}} by your user name, and {{<c>}}&lt;cluster&gt;{{</c>}} by the name of the Compute Canada cluster hostname (e.g. beluga, cedar, graham).)/

You can now open a browser and go to http://localhost:6006.

*Note: TensorBoard requires a lot of processing power. Please do not run it from the logging node. Only run it in a job (batch or interactive).*

* Comments & questions
