#+title: HPC with Python
#+description: Practice
#+colordes: #dc7309
#+slug: pt-04-hpc
#+weight: 4

#+BEGIN_simplebox
/Note on notation:/ {{<s>}}expressions between the ~<~ and ~>~ signs need to be replaced by the relevant information (without those signs).
#+END_simplebox

Whether you are using our training cluster (which is a virtual machine hosted in the cloud mimicking a Compute Canada cluster) or one of Compute Canada clusters (e.g. Cedar or Graham), when you ~ssh~ into the cluster, you "arrive" on the login node.

*Do not run jobs on this node*. To run your code, you need to start an interactive job or submit a job to Slurm (the job scheduler used by the Compute Canada clusters).

If you are not familiar with HPC, you can go through [[https://wgschool.netlify.app/hpc-menu/][the material of the HPC course we offered in this summer school]], [[https://westgrid.github.io/trainingMaterials/getting-started/#introduction-to-westgrid-compute-canada-and-hpc][the training resources on the WestGrid website]], and [[https://www.youtube.com/playlist?list=PLeCQbAbRSKR8gg6ZMFof1Hf9YF_-n31Ym][the Compute Canada introductory videos]].

This lesson goes over what we will do in this course, using our training cluster (of course, [[https://westgrid-ml.netlify.app/schoolremake/pt-03-local.html][if you run the code locally (on your computer)]], you can simply launch Python).

* Plots

Do not run code that displays plots on screen. Instead, have them written to files.

* Copy files to/from the cluster

** Few files

If you need to copy files to or from the cluster, you can use ~scp~.

***** From your computer

If you are in a local shell, run:

#+BEGIN_src sh
[local]$ scp </local/path/to/file> <user>@<hostname>:<path/in/cluster>
#+END_src

/(Replace {{<c>}}&lt;user&gt;{{</c>}} by your user name and {{<c>}}&lt;hostname&gt;{{</c>}} by the hostname (for this workshop: ~uu.c3.ca~).)/

***** From the cluster

If you are in a remote shell (through ~ssh~), run:

#+BEGIN_src sh
[remote]$ scp <user>@<hostname>:<cluster/path/to/file> </local/path>
#+END_src

** Large amount of data

Use [[https://docs.computecanada.ca/wiki/Globus][Globus]] for large data transfers.

* Start an interactive job

You start an interactive job with:

#+BEGIN_src sh
$ salloc --account=def-<user> --cpus-per-task=<n> --gres=gpu:<n> --mem=<mem> --time=<time>
#+END_src

Our training cluster does not have GPUs, so for this workshop, do not use the ~--gres=gpu:<n>~ option.

You also don't have to worry about the ~--account=def-<user>~ option (or, if you want, you can use ~--account=def-sponsor00~).

Our training cluster has a total of 60 CPUs on 5 compute nodes. Since there are many of you in this workshop, *please be very mindful when running interactive jobs*: if you request a lot of CPUs for a long time, the other workshop attendees won't be able to use the cluster anymore until your interactive job requested time ends (even if you aren't running any code).

Here are my suggestions so that we don't run into this problem:

- Only start interactive jobs when you need to understand what Python is doing (so where an interactive shell is really beneficial). [[https://westgrid-ml.netlify.app/schoolremake/pt-07-tensor.html][The practice lesson on Python tensors]] would be a good case for this if you want to play with tensors of various dimensions to familiarize yourself with them
- When running interactive jobs, only request 1 CPU (so ~--cpus-per-task=1~)
- Only request the time that you will really use (e.g. for [[https://westgrid-ml.netlify.app/schoolremake/pt-07-tensor.html][the lesson on Python tensors]], maybe 30 min seems reasonable)

For most things (i.e. more or less everything except [[https://westgrid-ml.netlify.app/schoolremake/pt-07-tensor.html][the practice lesson on Python tensors]]), submit a job to Slurm with a Python script.

#+BEGIN_simplebox
/Note:/ \\
In this workshop, we will submit jobs from ~/home~. This is fine on some of the Compute Canada clusters as well. Be aware however that [[https://docs.computecanada.ca/wiki/Running_jobs#Cluster_particularities][you are not allowed to do that on Cedar]]. Instead, on Cedar, you have to submit jobs from ~/scratch~ or ~/project~.
#+END_simplebox

* Submit a job to Slurm

** Job script

To submit a job to Slurm, you need to write an {{<b>}}sbatch{{</b>}} script.

Here is an example script:

#+BEGIN_src sh
#!/bin/bash
#SBATCH --job-name=<name>*			  # job name
#SBATCH --account=def-<user>
#SBATCH --time=<time>				  # max walltime in D-HH:MM or HH:MM:SS
#SBATCH --cpus-per-task=<number>      # number of cores
#SBATCH --gres=gpu:<type>:<number>    # type and number of GPU(s) per node
#SBATCH --mem=<mem>					  # max memory (default unit is MB) per node
#SBATCH --output=<file%j.out>*		  # file name for the output
#SBATCH --error=<file%j.err>*		  # file name for errors
					                  # %j gets replaced by the job number
#SBATCH --mail-user=<email_address>*
#SBATCH --mail-type=ALL*

# Load modules
# (Do not use this in our workshop since we aren't using GPUs)
# (Note: loading the Python module is not necessary when you active your virtual env)
# module load cudacore/.10.1.243 cuda/10 cudnn/7.6.5

# Create variable with the directory for your ML project
SOURCEDIR=~/<path/project/dir>

# Activate your Python virtual environment
source ~/env/bin/activate

# Transfer and extract data
mkdir $SLURM_TMPDIR/data
tar xf ~/projects/def-<user>/<data>.tar -C $SLURM_TMPDIR/data

# Run Python script on the data
python $SOURCEDIR/<mlscript>.py $SLURM_TMPDIR/data
#+END_src

/Notes:/

- If you compressed your data with {{<c>}}tar czf{{</c>}}, you need to extract it with {{<c>}}tar xzf{{</c>}}
- {{<c>}}SBATCH{{</c>}} options marked with a {{<c>}}*{{</c>}} are optional
- There are various other options for [[https://docs.computecanada.ca/wiki/Running_jobs#Email_notification][email notifications]].

** Job handling

**** Submit job

#+BEGIN_src sh
$ cd </dir/containing/job>
$ sbatch <jobscript>.sh
#+END_src

**** Check job status

#+BEGIN_src sh
$ sq
#+END_src

{{<b>}}PD{{</b>}} = pending\\
{{<b>}}R{{</b>}} = running

**** Cancel job

#+BEGIN_src sh
$ scancel <jobid>
#+END_src

**** Display efficiency measures of completed job

#+BEGIN_src sh
$ seff <jobid>
#+END_src

* Comments & questions
