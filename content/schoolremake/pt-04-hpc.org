#+title: HPC with Python
#+description: Practice
#+colordes: #dc7309
#+slug: pt-04-hpc
#+weight: 4

#+BEGIN_simplebox
/Note on notation:/ {{<s>}}expressions between the {{<c>}}&lt;{{</c>}} and {{<c>}}&gt;{{</c>}} signs need to be replaced by the relevant information (without those signs).
#+END_simplebox

Whether you are using our training cluster (which is a virtual machine hosted in the cloud mimicking a Compute Canada cluster) or one of Compute Canada clusters (e.g. Cedar or Graham), when you ssh into the cluster, you "arrive" on the login node.

*This node is not meant to run jobs*. To run some code, you need to submit a script to Slurm (the Compute Canada scheduler) or start an interactive job.

If you are not familiar with HPC, you can go through [[https://wgschool.netlify.app/hpc-menu/][the material of the HPC course we offered in this summer school]], [[https://westgrid.github.io/trainingMaterials/getting-started/#introduction-to-westgrid-compute-canada-and-hpc][the training resources on the WestGrid website]], and [[https://www.youtube.com/playlist?list=PLeCQbAbRSKR8gg6ZMFof1Hf9YF_-n31Ym][the Compute Canada introductory videos]].

[[][xxxx]] summarizes the workflow to run Python on the Compute Canada clusters.

This lesson goes over what we will do in this course, using our training cluster (of course, [[https://westgrid-ml.netlify.app/schoolremake/pt-03-local.html][if you run the code locally (on your computer)]], you can simply launch Python).

* Prepare your script

Throughout your script, make sure to have plots written to file and not displayed on screen.

You can develop your script directly in the cluster using an interactive job, or on your local machine (working on your machine at this stage, using very little data and few epochs—only to develop and test the code—is a convenient approach).

Ultimately, you will submit your script to {{<c>}}sbatch{{</c>}}.

* Copy files to the cluster

** Small files

If you need to copy files such as your script to the cluster, you can use {{<c>}}scp{{</c>}}.

***** From your computer

If you are in a local shell, run:

#+BEGIN_src sh
[local]$ scp </local/path/to/file>  <user>@<cluster>.computecanada.ca:<path/in/cluster>
#+END_src

/(Replace {{<c>}}&lt;user&gt;{{</c>}} by your user name and {{<c>}}&lt;cluster&gt;{{</c>}} by the name of the Compute Canada cluster hostname (e.g. beluga, cedar, graham).)/

***** From the cluster

If you are in a remote shell (through ssh), run:

#+BEGIN_src sh
[cluster]$ scp <user>@<cluster>.computecanada.ca:<cluster/path/to/file>  </local/path>
#+END_src

** Data

Use [[https://docs.computecanada.ca/wiki/Globus][Globus]] for large data transfers.

* Start an interactive job

You start an interactive job with:

#+BEGIN_src sh
$ salloc --account=def-<user> --cpus-per-task=<n> --mem=<mem> --time=<time>
#+END_src

--account=def-sponsor00

Our training cluster has a total of 60 CPUs on 5 compute nodes. Since there are many of you in this workshop, *please be very mindful when running interactive jobs*: if you request a lot of CPUs for a long time, the other workshop attendees won't be able to use the cluster anymore until your interactive job requested time ends (even if you aren't running any code).

Here are my suggestions so that we don't run into this problem:

- Only start interactive jobs when you need to understand what Python is doing and are not running anything computationally intensive. [[https://westgrid-ml.netlify.app/schoolremake/pt-07-tensor.html][The practice lesson on Python tensors]] would be a good case for this
- When running interactive jobs, only request 1 CPU (so =--cpus-per-task=1=)
- Only request the time that you will really use

When you want to run models (later lessons), write a python script and submit a job to Slurm.

#+BEGIN_simplebox
=--gres=gpu:<n>=
#+END_simplebox

* Submit a job to Slurm

** Job script

To submit a job to Slurm (the job scheduler used by the Compute Canada clusters), you need to write an {{<b>}}sbatch{{</b>}} script. Here is an example script:

#+BEGIN_src sh
#!/bin/bash
#SBATCH --job-name=<name>*			  # job name
#SBATCH --account=def-<user>
#SBATCH --time=<time>				  # max walltime in D-HH:MM or HH:MM:SS
#SBATCH --cpus-per-task=<number>      # number of cores
#SBATCH --gres=gpu:<type>:<number>    # type and number of GPU(s) per node
#SBATCH --mem=<mem>					  # max memory (default unit is MB) per node
#SBATCH --output=<file%j.out>*		  # file name for the output
#SBATCH --error=<file%j.err>*		  # file name for errors
					                  # %j gets replaced by the job number
#SBATCH --mail-user=<email_address>*
#SBATCH --mail-type=ALL*

# Load modules
module load python/3.8.2 cudacore/.10.1.243 cuda/10 cudnn/7.6.5

# Create variable with the directory for your ML project
SOURCEDIR=~/<path/project/dir>

# Create and activate a virtual environment on compute node
virtualenv --no-download $SLURM_TMPDIR/env
source $SLURM_TMPDIR/env/bin/activate

# Install Python packages
pip install --no-index -r $SOURCEDIR/requirements.txt

# Transfer and extract data
mkdir $SLURM_TMPDIR/data
tar xf ~/projects/def-<user>/<data>.tar -C $SLURM_TMPDIR/data

# Run Python script on the data
python $SOURCEDIR/<mlscript>.py $SLURM_TMPDIR/data
#+END_src

/Notes:/

- If you compressed your data with {{<c>}}tar czf{{</c>}}, you need to extract it with {{<c>}}tar xzf{{</c>}}
- {{<c>}}SBATCH{{</c>}} options marked with a {{<c>}}*{{</c>}} are optional
- There are various other options for [[https://docs.computecanada.ca/wiki/Running_jobs#Email_notification][email notifications]].

** Job handling

**** Submit job

#+BEGIN_src sh
$ cd </dir/containing/job>
$ sbatch <jobscript>.sh
#+END_src

**** Check job status

#+BEGIN_src sh
$ sq
#+END_src

{{<b>}}PD{{</b>}} = pending\\
{{<b>}}R{{</b>}} = running

**** Cancel job

#+BEGIN_src sh
$ scancel <jobid>
#+END_src

**** Display efficiency measures of completed job

#+BEGIN_src sh
$ seff <jobid>
#+END_src

* Comments & questions
