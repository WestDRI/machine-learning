#+title: Additional notes on running ML jobs on Compute Canada clusters
#+description: Reading
#+colordes: #538cc6
#+slug: pt-16-mlhpc
#+weight: 16

* GPU(s)

*** GPU type

Several Compute Canada clusters have GPUs. Their numbers and types differ:

{{<img src="/img/school/cc_gpu.png" title="" width="%" line-height="0.5rem">}}
from <a href="https://docs.computecanada.ca/wiki/Using_GPUs_with_Slurm">Compute Canada Wiki</a>
<br><br>
{{</img>}}

The default is {{<b>}}12G P100{{</b>}}, but you can request another type with {{<c>}}SBATCH --gres=gpu:&lt;type&gt;:&lt;number&gt;{{</c>}}

*** Number of GPU(s)

#+BEGIN_export html
<font color="#bf540c">Try running your model on <b>one</b> GPU first.</font>
#+END_export

It is very likely that you do not need more than one GPU. Asking for more than you need will greatly increase your waiting time until your job is run. The session on [[https://westgrid-ml.netlify.app/school/pt-11-distributed.html][distributed computing with PyTorch]] will help you see when you might benefit from several GPUs. In any event, you should test your model before asking for several GPUs.

*** CPU/GPU ratio

Here are Compute Canada recommendations:

*Béluga*:\\
No more than 10 CPU per GPU

*Cedar*:\\
P100 GPU: no more than 6 CPU per GPU\\
V100 GPU: no more than 8 CPU per GPU

*Graham*:\\
No more than 16 CPU per GPU

* Test your code in an interactive job before submitting a big job to Slurm

** Start an interactive job

/Example:/

#+BEGIN_src sh
$ salloc --account=def-<user> --gres=gpu:1 --cpus-per-task=6 --mem=32000 --time=1:00
#+END_src

** Try to run your code

Create a temporary data directory in {{<b>}}$SLURM_TMPDIR{{</b>}}:

#+BEGIN_src sh
(env) $ mkdir $SLURM_TMPDIR/data
#+END_src

Extract the data into it:

#+BEGIN_src sh
(env) $ tar xf ~/projects/def-<user>/<data>.tar -C $SLURM_TMPDIR/data
#+END_src

Play in Python to test your code:

#+BEGIN_src sh
(env) $ python
#+END_src

#+BEGIN_src python
>>> import torch
>>> ...
#+END_src

/Note:/ {{<s>}}if you want to exit the virtual environment:

#+BEGIN_src sh
(env) $ deactivate
#+END_src

* Checkpoints

Long jobs should have a checkpoint at least every 24 hours. This ensures that an outage won't lead to days of computation lost and it will help get the job started sooner by the scheduler.

For instance, you might want to have checkpoints every {{<b>}}n{{</b>}} epochs (choose {{<b>}}n{{</b>}} so that {{<b>}}n{{</b>}} epochs take less than 24 hours to run).

In PyTorch, you can create dictionaries with all the information necessary and save them as {{<b>}}.tar{{</b>}} files with {{<c>}}torch.save(){{</c>}}. You can then load them back with {{<c>}}torch.load(){{</c>}}.

The information you want to save in each checkpoint includes the model’s state_dict, the optimizer’s state_dict, the epoch at which you stopped, the latest training loss, and anything else needed to restart training where you left off.

/Example:/

Saving a checkpoint during training could look something like this:

#+BEGIN_src python
torch.save({
    'epoch': <last epoch run>,
    'model_state_dict': net.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'loss': <latest loss>,
}, <path/to/file.tar>)
#+END_src

To restart, initialize the model and optimizer, load the dictionary, and resume training:

#+BEGIN_src python
# Initialize the model and optimizer
model = <your model>
optimizer = <your optimizer>

# Load the dictionary
checkpoint = torch.load(<path/to/file.tar>)
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
epoch = checkpoint['epoch']
loss = checkpoint['loss']

# Resume training
model.train()
#+END_src

* Running several similar jobs

A number of ML tasks (e.g. [[https://en.wikipedia.org/wiki/Hyperparameter_optimization][hyperparameter optimization]]) require running several instances of similar jobs. Grouping them into a single job with [[https://docs.computecanada.ca/wiki/GLOST][GLOST]] or [[https://docs.computecanada.ca/wiki/GNU_Parallel][GNU Parallel]] reduces the stress on the scheduler.

* Comments & questions
