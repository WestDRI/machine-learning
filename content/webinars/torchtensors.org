#+title: Everything you wanted to know (and more) about PyTorch tensors
#+slug: torchtensors
#+date: 2022-01-19
#+place: 60 min live webinar

*** /Abstract/

#+BEGIN_definition
Before information can be fed to artificial neural networks (ANN), it needs to be converted to a form ANN can process: floating point numbers. Indeed, you don't pass a sentence or an image through an ANN; you input numbers representing a sequence of words or pixel values.

All these floating point numbers need to be stored in a data structure. The most suited structure is multidimensional (to hold several layers of information) and since all data is of the same type, it is an array.

Python already has several multidimensional array structures—the most popular of which being NumPy's ndarray—but the particularities of deep learning call for special characteristics: ability to run operations on GPUs and/or in a distributed fashion, as well as the ability to keep track of computation graphs for automatic differentiation.

The PyTorch tensor is a Python data structure with these characteristics that can also easily be converted to/from NumPy's ndarray and integrates well with other Python libraries such as Pandas.

In this webinar, suitable for users of all levels, we will have a deep look at this data structure and go much beyond a basic introduction.

In particular, we will:
- see how tensors are stored in memory
- look at the metadata which allows this efficient memory storage
- cover the basics of working with tensors (indexing, vectorized operations...)
- move tensors to/from GPUs
- convert tensors to/from NumPy ndarrays
- see how tensors work in distributed frameworks
- see how linear algebra can be done with PyTorch tensors
#+END_definition

# Comments from Alex:

# - multi-processor parallelism? do they do threads? how about distributed-memory processing?
# - working demos on CC clusters

# It makes sense to talk about these only when using PyTorch tensors for things like linear algebra and vectorized calculations.
# Since linear algebra can be CPU-intensive (and I know in PyTorch you can run it on a GPU as well), can you speed up linear algebra calculations by using multiple threads, all processing the same set of tensors in shared memory? When your tensors become large and cannot fit in shared memory on one node, can you break them into pieces and distribute across several cluster nodes, and then process these pieces (e.g. solve a linear system) as if it were a single object?

# For example, consider solving a dense linear system, where the square matrix is of size 250,000^2. Just to store it in single precision, you will need 233GB memory.

* Slides

#+BEGIN_export html
<figure style="display: table;">
  <div class="row">
	<div style="float: left; width: 65%">
	  <img style="border-style: solid; border-color: black" src="/img/torchtensors_webinar_slides.png">
	</div>
	<div style="float: left; width: 35%">
	  <div style="padding: 20% 0 0 15%;">
        <a href="https://westgrid-slides.netlify.app/torchtensors_webinar/#/" target="_blank">Web version</a>
	  </div>
	  <div style="padding: 5% 0 0 15%;">
	  <a href="/pdf/torchtensors_webinar.pdf">Pdf version</a>
	  </div>
	</div>
  </div>
</figure>
#+END_export

#+BEGIN_note
Both versions open in a new tab.\\
In the web version, use the left and right arrows to navigate the slides.
#+END_note

* Video

Coming soon.

* Comments & questions
